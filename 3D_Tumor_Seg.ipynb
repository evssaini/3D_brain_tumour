{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**To install packages**"
      ],
      "metadata": {
        "id": "sZ8AjGP8yyQ1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnRxSvbDHsqS"
      },
      "outputs": [],
      "source": [
        "!pip install nibabel -q\n",
        "!pip install scikit-learn -q\n",
        "!pip install tqdm -q\n",
        "!pip install split-folders -q\n",
        "!pip install torchinfo -q\n",
        "!pip install segmentation-models-pytorch-3d -q\n",
        "!pip install livelossplot -q\n",
        "!pip install torchmetrics -q\n",
        "!pip install tensorboard -q\n",
        "!apt-get install tree"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import splitfolders\n",
        "from tqdm import tqdm\n",
        "import nibabel as nib\n",
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import torchvision.transforms as transforms\n",
        "from torch.cuda import amp\n",
        "\n",
        "from torchmetrics import MeanMetric\n",
        "from torchmetrics.classification import MulticlassAccuracy\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchinfo import summary\n",
        "import gc\n",
        "\n",
        "import segmentation_models_pytorch_3d as smp\n",
        "\n",
        "from livelossplot import PlotLosses\n",
        "from livelossplot.outputs import MatplotlibPlot, ExtremaPrinter\n",
        "from torch.optim import AdamW\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib.colorbar import Colorbar"
      ],
      "metadata": {
        "id": "o6arrzA2QDxe"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mount Drive to acess dataset**"
      ],
      "metadata": {
        "id": "He40LoNiy5Ii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ahFRtGDvQ9NT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(SEED):\n",
        "   np.random.seed(SEED)\n",
        "   torch.manual_seed(SEED)\n",
        "   torch.cuda.manual_seed_all(SEED)\n",
        "   torch.backends.cudnn.deterministic = True\n",
        "   torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def get_default_device():\n",
        "   gpu_available = torch.cuda.is_available()\n",
        "   return torch.device('cuda' if gpu_available else 'cpu'), gpu_available"
      ],
      "metadata": {
        "id": "dprwt0VwylcQ"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataclass(frozen=True)\n",
        "class TrainingConfig:\n",
        "   BATCH_SIZE:      int = 5\n",
        "   EPOCHS:          int = 50\n",
        "   LEARNING_RATE: float = 5e-5\n",
        "   CHECKPOINT_DIR:  str = os.path.join('/content/drive/MyDrive/MSc_Project', '3D_Brain_Images')\n",
        "   NUM_WORKERS:     int = 4"
      ],
      "metadata": {
        "id": "VT0jxMJI4W5E"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Sample Preprocessing**"
      ],
      "metadata": {
        "id": "sMsSVkvR9TOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "Dataset_path = '/content/drive/MyDrive/MSc_Project/BraTS2021_Training_Data'\n",
        "print(f'Total Files: ', len(os.listdir(Dataset_path)))\n"
      ],
      "metadata": {
        "id": "sPhKwqX94bZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To check file structure**"
      ],
      "metadata": {
        "id": "qn465IjaAHqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tree -L 2 \"/content/drive/MyDrive/MSc_Project/BraTS2021_Training_Data/BraTS2021_00000\""
      ],
      "metadata": {
        "id": "04xEzB8M-enM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load NifTi Images using nib.load()**\n",
        "\n",
        "returns a numpy array"
      ],
      "metadata": {
        "id": "yCNhGg_AAgpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_image_flair_3d = nib.load(os.path.join(Dataset_path, 'BraTS2021_00000', 'BraTS2021_00000_flair.nii.gz')).get_fdata()\n",
        "print(f'Original max value:', sample_image_flair_3d.max())\n",
        "\n",
        "# reshape the 3d image to 2d iamge for scaling\n",
        "sample_image_flair_2d = sample_image_flair_3d.reshape(-1,1)"
      ],
      "metadata": {
        "id": "TLIK2Hhq_KWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One sample taken for understanding how preprocessing would work on the whole dataset**"
      ],
      "metadata": {
        "id": "idwHFkEQGxTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply scaling\n",
        "sample_image_flair_scaled = scaler.fit_transform(sample_image_flair_2d)\n",
        "\n",
        "# Reshape it back to original form\n",
        "sample_image_flair_scaled = sample_image_flair_scaled.reshape(sample_image_flair_3d.shape)\n",
        "\n",
        "print(f'Scaled max value:', sample_image_flair_scaled.max())\n",
        "print(f'Shape of scaled image:', sample_image_flair_scaled.shape)"
      ],
      "metadata": {
        "id": "8tE4uq-0Gj8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use of seg MRI modality to see the classes**"
      ],
      "metadata": {
        "id": "vf7cw4Y1LBJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_image_seg_3d = nib.load(os.path.join(Dataset_path, 'BraTS2021_00000', 'BraTS2021_00000_seg.nii.gz')).get_fdata()\n",
        "sample_image_seg_3d = sample_image_seg_3d.astype(np.uint8)\n",
        "\n",
        "print(f'Unique class in the mask', np.unique(sample_image_seg_3d))"
      ],
      "metadata": {
        "id": "XhkBJOeIJuLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To choose a random slice, manual paitent id and visualize it per modality for better insights**"
      ],
      "metadata": {
        "id": "ub5_3KXFL3iv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_image_t1_3d = nib.load(os.path.join(Dataset_path, 'BraTS2021_00000', 'BraTS2021_00000_t1.nii.gz')).get_fdata()\n",
        "sample_image_t1ce_3d = nib.load(os.path.join(Dataset_path, 'BraTS2021_00000', 'BraTS2021_00000_t1ce.nii.gz')).get_fdata()\n",
        "sample_image_t2_3d = nib.load(os.path.join(Dataset_path, 'BraTS2021_00000', 'BraTS2021_00000_t2.nii.gz')).get_fdata()"
      ],
      "metadata": {
        "id": "dWiNxFwELnPy"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# random slice between 0-154\n",
        "n_slice = random.randint(0,sample_image_seg_3d.shape[2]-1)\n",
        "print(f'random slice number:', n_slice)\n",
        "plt.figure(figsize=(12,8))\n",
        "\n",
        "plt.subplot(231)\n",
        "plt.imshow(sample_image_flair_3d[:,:,n_slice], cmap='gray')\n",
        "plt.title('Flair Image')\n",
        "\n",
        "plt.subplot(232)\n",
        "plt.imshow(sample_image_t1_3d[:,:,n_slice], cmap='gray')\n",
        "plt.title('T1 Image')\n",
        "\n",
        "plt.subplot(233)\n",
        "plt.imshow(sample_image_t1ce_3d[:,:,n_slice], cmap='gray')\n",
        "plt.title('T1ce Image')\n",
        "\n",
        "plt.subplot(234)\n",
        "plt.imshow(sample_image_t2_3d[:,:,n_slice], cmap='gray')\n",
        "plt.title('T2 Image')\n",
        "\n",
        "plt.subplot(235)\n",
        "plt.imshow(sample_image_seg_3d[:,:,n_slice])\n",
        "plt.title('Seg Image')\n",
        "\n",
        "\n",
        "plt.subplot(236)\n",
        "plt.imshow(sample_image_seg_3d[:,:,n_slice], cmap = 'gray')\n",
        "plt.title('Mask Gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sxBbQMats1OF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To see all modalities in loop, from all 3 axes**"
      ],
      "metadata": {
        "id": "eGHvVcUH47uQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "volume = np.zeros((240, 240, 155))  # A blank MRI\n",
        "n_slice = random.randint(0, 154)\n",
        "slice_2d = volume[:, :, n_slice]    # 2D image at the randomly chosen depth\n"
      ],
      "metadata": {
        "id": "lt1MPMAe_A-1"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose slices near the center of each dimension\n",
        "y, x, z = sample_image_flair_3d.shape[0]//2, volume.shape[1]//2, volume.shape[2]//2\n",
        "\n",
        "plt.figure(figsize=(18, 5))\n",
        "\n",
        "# Axial (top-down)\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(sample_image_flair_3d[:, :, z], cmap='gray')\n",
        "plt.title(f'Axial (Z={z}), X-Y')\n",
        "plt.axis('off')\n",
        "\n",
        "# Coronal (front-back)\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(sample_image_flair_3d[:, y, :], cmap='gray')\n",
        "plt.title(f'Coronal (Y={y}), X-Z')\n",
        "plt.axis('off')\n",
        "\n",
        "# Sagittal (side)\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(sample_image_flair_3d[x, :, :], cmap='gray')\n",
        "plt.title(f'Sagittal (X={x}), Y-Z')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XtRo3hecwh5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_x = np.stack([sample_image_flair_3d,sample_image_t1ce_3d,sample_image_t2_3d],axis=3)\n",
        "print(\"Shape of Combined x \", combined_x.shape)"
      ],
      "metadata": {
        "id": "wqH4Msp-A-_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_x = combined_x[56:184, 56:184, 13:141]\n",
        "print(\"Shape after cropping: \", combined_x.shape)\n",
        "\n",
        "sample_mask_c = sample_image_seg_3d[56:184,56:184, 13:141]\n",
        "print(\"Mask shape after cropping: \", sample_mask_c.shape)"
      ],
      "metadata": {
        "id": "jddZCOlLSiuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remap class value 4 to 3\n",
        "sample_mask_c[sample_mask_c == 4] = 3\n",
        "\n",
        "# Now apply one_hot encoding with num_classes=4\n",
        "sample_mask_cat  = F.one_hot(torch.tensor(sample_mask_c, dtype = torch.long), num_classes = 4)\n",
        "print(\"Shape after one-hot encoding: \", sample_mask_cat.shape)"
      ],
      "metadata": {
        "id": "20DT5M5RS-Bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t1ce_list = sorted(glob.glob(f\"{Dataset_path}/*/*t1ce.nii.gz\"))\n",
        "t2_list = sorted(glob.glob(f\"{Dataset_path}/*/*t2.nii.gz\"))\n",
        "flair_list = sorted(glob.glob(f\"{Dataset_path}/*/*flair.nii.gz\"))\n",
        "mask_list = sorted(glob.glob(f\"{Dataset_path}/*/*seg.nii.gz\"))\n",
        "\n",
        "print(\"t1ce list: \", len(t1ce_list))\n",
        "print(\"t2 list: \", len(t2_list))\n",
        "print(\"flair list: \", len(flair_list))\n",
        "print(\"Mask list: \", len(mask_list))"
      ],
      "metadata": {
        "id": "MHqU-3JlUES9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Data Preprocessing**"
      ],
      "metadata": {
        "id": "OYop4w0wS1Gx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to show progress bar\n",
        "for idx in tqdm(range(len(t2_list)), desc='Preparing to stack, crop and save', unit='file'):\n",
        "  temp_image_t1ce = nib.load(t1ce_list[idx]).get_fdata()\n",
        "  temp_image_t1ce = scaler.fit_transform(temp_image_t1ce.reshape(-1, temp_image_t1ce.shape[-1])).reshape(temp_image_t1ce.shape)\n",
        "\n",
        "  temp_image_t2 = nib.load(t2_list[idx]).get_fdata()\n",
        "  temp_image_t2 = scaler.fit_transform(temp_image_t2.reshape(-1, temp_image_t2.shape[-1])).reshape(temp_image_t2.shape)\n",
        "\n",
        "  temp_image_flair = nib.load(flair_list[idx]).get_fdata()\n",
        "  temp_image_flair = scaler.fit_transform(temp_image_flair.reshape(-1, temp_image_flair.shape[-1])).reshape(temp_image_flair.shape)\n",
        "\n",
        "  temp_seg_mask = nib.load(mask_list[idx]).get_fdata()\n",
        "\n",
        "  temp_comb_images = np.stack([temp_image_flair, temp_image_t1ce, temp_image_t2], axis=3)\n",
        "  temp_comb_images = temp_comb_images[56:184, 56:184, 13:141]\n",
        "  temp_seg_mask = temp_seg_mask[56:184, 56:184, 13:141]\n",
        "\n",
        "  temp_seg_mask[temp_seg_mask == 4] = 3\n",
        "\n",
        "  val, counts = np.unique(temp_seg_mask, return_counts=True)\n",
        "\n",
        "# if a volume has less than 1% of mask, simplw ignore to reduce the computation.\n",
        "  if(1 - (counts[0] / counts.sum())) > 0.01:\n",
        "    temp_seg_mask = F.one_hot(torch.tensor(temp_seg_mask, dtype=torch.long), num_classes=4)\n",
        "    out_dir_images = \"/content/drive/MyDrive/MSc_Project/BraTS2021_Preprocessed/input_data_3channels/images\"\n",
        "    out_dir_masks  = \"/content/drive/MyDrive/MSc_Project/BraTS2021_Preprocessed/input_data_3channels/masks\"\n",
        "    os.makedirs(out_dir_images, exist_ok=True)\n",
        "    os.makedirs(out_dir_masks, exist_ok=True)\n",
        "    np.save(\n",
        "        os.path.join(out_dir_images, f\"image_{idx}.npy\"),\n",
        "        temp_comb_images,\n",
        "    )\n",
        "    np.save(\n",
        "        os.path.join(out_dir_masks, f\"mask_{idx}.npy\"),\n",
        "        temp_seg_mask,\n",
        "    )\n",
        "\n",
        "  else:\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "dbA5eVC7Spyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_folder = \"/content/drive/MyDrive/MSc_Project/BraTS2021_Preprocessed/input_data_3channels/images\"\n",
        "print(len(os.listdir(images_folder)))\n",
        "\n",
        "masks_folder = \"/content/drive/MyDrive/MSc_Project/BraTS2021_Preprocessed/input_data_3channels/masks\"\n",
        "print(len(os.listdir(masks_folder)))"
      ],
      "metadata": {
        "id": "OTOwkl14NXWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split the data into folders\n",
        "input_folder = \"/content/drive/MyDrive/MSc_Project/BraTS2021_Preprocessed/input_data_3channels/\"\n",
        "\n",
        "output_folder = \"/content/drive/MyDrive/MSc_Project/BraTS2021_Preprocessed/input_data_split/\"\n",
        "\n",
        "splitfolders.ratio(\n",
        "    input_folder, output_folder, seed=42, ratio=(0.75, 0.15, 0.10), group_prefix=None\n",
        ")\n",
        "print('Sucess!')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OPG6c6O9JTaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataLoader"
      ],
      "metadata": {
        "id": "3_d9L7ZAj9cx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset_Brats21(Dataset):\n",
        "    def __init__(self, img_dir, mask_dir, normalization=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.img_dir = img_dir\n",
        "        self.mask_dir = mask_dir\n",
        "\n",
        "        self.img_list = sorted(os.listdir(img_dir))\n",
        "        self.mask_list = sorted(os.listdir(mask_dir))\n",
        "\n",
        "\n",
        "        if len(self.img_list) != len(self.mask_list):\n",
        "            raise ValueError(f\"Number of images ({len(self.img_list)}) and masks ({len(self.mask_list)}) do not match in {img_dir} and {mask_dir}\")\n",
        "\n",
        "        self.normalization = normalization\n",
        "\n",
        "\n",
        "        if self.normalization:\n",
        "          # mean and std for t1ce, flair, t2\n",
        "            self.mean = torch.tensor([0.5]*3)\n",
        "            self.std = torch.tensor([0.5]*3)\n",
        "\n",
        "\n",
        "    def load_file(self, filepath, retries=5, delay=1):\n",
        "        # loads a file with retries to handle transient errors\n",
        "        for i in range(retries):\n",
        "            try:\n",
        "\n",
        "                data = np.load(filepath, allow_pickle=False)\n",
        "                if data.size == 0:\n",
        "                    raise ValueError(f\"Loaded empty data from {filepath}\")\n",
        "                return data\n",
        "            except (OSError, ValueError) as e:\n",
        "                print(f\"Attempt {i+1}/{retries} failed to load {filepath}: {e}\")\n",
        "                if i < retries - 1:\n",
        "                    time.sleep(delay)\n",
        "                else:\n",
        "                    raise e\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = os.path.join(self.img_dir, self.img_list[idx])\n",
        "        mask_path = os.path.join(self.mask_dir, self.mask_list[idx])\n",
        "\n",
        "        try:\n",
        "\n",
        "            image = self.load_file(image_path)\n",
        "            mask = self.load_file(mask_path)\n",
        "\n",
        "            # convert to torch tensors and permute axes to C, D, H, W format\n",
        "            image = torch.from_numpy(image).permute(3, 0, 1, 2).float()\n",
        "            mask = torch.from_numpy(mask).permute(3, 0, 1, 2).float()\n",
        "\n",
        "            # normalize the image per channel\n",
        "            if self.normalization:\n",
        "                # reshape mean and std\n",
        "                mean = self.mean.view(-1, 1, 1, 1)\n",
        "                std = self.std.view(-1, 1, 1, 1)\n",
        "                image = (image - mean) / std\n",
        "\n",
        "\n",
        "            return image, mask\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping sample {self.img_list[idx]} due to error: {e}\")\n",
        "            raise e\n",
        "\n",
        "train_img_dir = \"/content/drive/MyDrive/MSc_Project/BraTS2021_Preprocessed/input_data_split/train/images\"\n",
        "train_mask_dir = \"/content/drive/MyDrive/MSc_Project/BraTS2021_Preprocessed/input_data_split/train/masks\"\n",
        "\n",
        "val_img_dir = \"/content/drive/MyDrive/MSc_Project/BraTS2021_Preprocessed/input_data_split/val/images\"\n",
        "val_mask_dir = \"/content/drive/MyDrive/MSc_Project/BraTS2021_Preprocessed/input_data_split/val/masks\"\n",
        "\n",
        "test_img_dir = \"/content/drive/MyDrive/MSc_Project/BraTS2021_Preprocessed/input_data_split/test/images\"\n",
        "test_mask_dir = \"/content/drive/MyDrive/MSc_Project/BraTS2021_Preprocessed/input_data_split/test/masks\"\n",
        "\n",
        "train_dataset = Dataset_Brats21(train_img_dir, train_mask_dir, normalization=True)\n",
        "val_dataset = Dataset_Brats21(val_img_dir, val_mask_dir, normalization=True)\n",
        "test_dataset = Dataset_Brats21(test_img_dir, test_mask_dir, normalization=True)\n",
        "\n",
        "# dataset statistics\n",
        "print(\"Total Training Samples: \", len(train_dataset))\n",
        "print(\"Total Val Samples: \", len(val_dataset))\n",
        "print(\"Total Test Samples: \", len(test_dataset))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=5, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=5, shuffle=False, num_workers=4)\n",
        "\n",
        "images, masks = next(iter(train_loader))\n",
        "print(f\"Train Image batch shape: {images.shape}\")\n",
        "print(f\"Train Mask batch shape: {masks.shape}\")\n",
        "\n",
        "images, masks = next(iter(val_loader))\n",
        "print(f\"Val Image batch shape: {images.shape}\")\n",
        "print(f\"Val Mask batch shape: {masks.shape}\")\n",
        "\n",
        "images, masks = next(iter(test_loader))\n",
        "print(f\"Test Image batch shape: {images.shape}\")\n",
        "print(f\"Test Mask batch shape: {masks.shape}\")\n"
      ],
      "metadata": {
        "id": "Ae6-Ry2hetwW",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building a 3D U-Net Model**"
      ],
      "metadata": {
        "id": "9lFZQqKMWIn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def DoubleConv(in_channels, out_channels):\n",
        "   return nn.Sequential(\n",
        "       nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "       nn.BatchNorm3d(out_channels),\n",
        "       nn.ReLU(inplace=False),\n",
        "       nn.Dropout(0.1 if out_channels <= 32 else 0.2 if out_channels <= 128 else 0.3),\n",
        "       nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "       nn.BatchNorm3d(out_channels),\n",
        "       nn.ReLU(inplace=False)\n",
        "   )"
      ],
      "metadata": {
        "id": "nEX3GH4Zh-qy"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Unet(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "       super().__init__()\n",
        "\n",
        "       # Encoder\n",
        "       self.conv1 = DoubleConv(in_channels=in_channels, out_channels=16)\n",
        "       self.pool1 = nn.MaxPool3d(kernel_size=2)\n",
        "\n",
        "       self.conv2 = DoubleConv(in_channels=16, out_channels=32)\n",
        "       self.pool2 = nn.MaxPool3d(kernel_size=2)\n",
        "\n",
        "       self.conv3 = DoubleConv(in_channels=32, out_channels=64)\n",
        "       self.pool3 = nn.MaxPool3d(kernel_size=2)\n",
        "\n",
        "       self.conv4 = DoubleConv(in_channels=64, out_channels=128)\n",
        "       self.pool4 = nn.MaxPool3d(kernel_size=2)\n",
        "\n",
        "       # Bottleneck\n",
        "       self.conv5 = DoubleConv(in_channels=128, out_channels=256)\n",
        "\n",
        "       # Decoder\n",
        "       self.upconv6 = nn.ConvTranspose3d(in_channels=256, out_channels=128, kernel_size=2, stride=2)\n",
        "       self.conv6 = DoubleConv(in_channels=256, out_channels=128)\n",
        "\n",
        "       self.upconv7 = nn.ConvTranspose3d(in_channels=128, out_channels=64, kernel_size=2, stride=2)\n",
        "       self.conv7 = DoubleConv(in_channels=128, out_channels=64)\n",
        "\n",
        "       self.upconv8 = nn.ConvTranspose3d(in_channels=64, out_channels=32, kernel_size=2, stride=2)\n",
        "       self.conv8 = DoubleConv(in_channels=64, out_channels=32)\n",
        "\n",
        "       self.upconv9 = nn.ConvTranspose3d(in_channels=32, out_channels=16, kernel_size=2, stride=2)\n",
        "       self.conv9 = DoubleConv(in_channels=32, out_channels=16)\n",
        "\n",
        "       self.out_conv = nn.Conv3d(in_channels=16, out_channels=out_channels, kernel_size=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "       # Contracting path\n",
        "       x1 = self.conv1(x)\n",
        "       x2 = self.pool1(x1)\n",
        "\n",
        "       x3 = self.conv2(x2)\n",
        "       x4 = self.pool2(x3)\n",
        "\n",
        "       x5 = self.conv3(x4)\n",
        "       x6 = self.pool3(x5)\n",
        "\n",
        "       x7 = self.conv4(x6)\n",
        "       x8 = self.pool4(x7)\n",
        "\n",
        "       x9 = self.conv5(x8)\n",
        "\n",
        "       # Expansive path\n",
        "       x10 = self.upconv6(x9)\n",
        "       x10 = torch.cat([x10,x7], dim=1) # skip connections\n",
        "       x10 = self.conv6(x10)\n",
        "\n",
        "       x11 = self.upconv7(x10)\n",
        "       x11 = torch.cat([x11,x5], dim=1)\n",
        "       x11 = self.conv7(x11)\n",
        "\n",
        "       x12 = self.upconv8(x11)\n",
        "       x12 = torch.cat([x12,x3], dim=1)\n",
        "       x12 = self.conv8(x12)\n",
        "\n",
        "       x13 = self.upconv9(x12)\n",
        "       x13 = torch.cat([x13,x1], dim=1)\n",
        "       x13 = self.conv9(x13)\n",
        "\n",
        "       x14 = self.out_conv(x13)\n",
        "\n",
        "       return x14"
      ],
      "metadata": {
        "id": "MrGT3WQRWcQQ"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining Losses and Optimizers from Segmentation**"
      ],
      "metadata": {
        "id": "cbMb9IkybxmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dice Loss\n",
        "dice_loss = smp.losses.DiceLoss(\n",
        "   mode=\"multiclass\",\n",
        "   classes=None,\n",
        "   log_loss=False,\n",
        "   from_logits=True,\n",
        "   smooth=1e-5,\n",
        "   ignore_index=None,\n",
        "   eps=1e-7\n",
        ")\n",
        "\n",
        "# Focal Loss\n",
        "focal_loss = smp.losses.FocalLoss(\n",
        "   mode=\"multiclass\",\n",
        "   alpha=0.25,\n",
        "   gamma=2.0\n",
        ")\n",
        "\n",
        "def combined_loss(output, target):\n",
        "   loss1 = dice_loss(output, target)\n",
        "   loss2 = focal_loss(output, target)\n",
        "   return loss1 + loss2"
      ],
      "metadata": {
        "id": "m24f4lkJYoK4"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checkpoint to based on valid loss**"
      ],
      "metadata": {
        "id": "mj30VXmWcHNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_checkpoint_dir(checkpoint_dir):\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "    try:\n",
        "        num_versions = [\n",
        "            int(i.split(\"_\")[-1]) for i in os.listdir(checkpoint_dir) if \"version\" in i\n",
        "        ]\n",
        "        version_num = max(num_versions) + 1\n",
        "\n",
        "    except:\n",
        "        version_num = 0\n",
        "\n",
        "    version_dir = os.path.join(checkpoint_dir, \"version_\" + str(version_num))\n",
        "    os.makedirs(version_dir)\n",
        "\n",
        "    print(f\"Checkpoint directory: {version_dir}\")\n",
        "    return version_dir\n",
        "\n",
        "seed_everything(SEED = 42)\n",
        "\n",
        "DEVICE, GPU_AVAILABLE  = get_default_device()\n",
        "print(DEVICE)\n",
        "\n",
        "CKPT_DIR = create_checkpoint_dir(TrainingConfig.CHECKPOINT_DIR)\n",
        "\n",
        "from torch.optim import AdamW\n",
        "\n",
        "optimizer = AdamW(\n",
        "   model.parameters(),\n",
        "   lr=TrainingConfig.LEARNING_RATE,\n",
        "   weight_decay=1e-2,                # Regularization to avoid overfitting\n",
        "   amsgrad=True                      # Optional AMSGrad variant\n",
        ")"
      ],
      "metadata": {
        "id": "mf_uPEtTcE2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train the model**"
      ],
      "metadata": {
        "id": "ELkRsbDhiLGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "def logits_to_probs(logits):\n",
        "    # convert raw network outputs to soft-max probabilities\n",
        "    return torch.softmax(logits, dim=1)\n",
        "\n",
        "# Train function\n",
        "def TrainModel(\n",
        "   model,\n",
        "   loader,\n",
        "   optimizer,\n",
        "   num_classes,\n",
        "   device=\"gpu\",\n",
        "   epoch_index=800,\n",
        "   total_epochs=50):\n",
        "\n",
        "\n",
        "   model.train()\n",
        "\n",
        "\n",
        "   loss_record = MeanMetric()\n",
        "\n",
        "   loader_len = len(loader)\n",
        "   iou_per_class = {i: MeanMetric() for i in range(num_classes)}\n",
        "   dice_per_class = {i: MeanMetric() for i in range(num_classes)}\n",
        "\n",
        "\n",
        "   with tqdm(total=loader_len, ncols=122) as tq:\n",
        "       tq.set_description(f\"Train ::  Epoch: {epoch_index}/{total_epochs}\")\n",
        "\n",
        "       for data, target in loader:\n",
        "           tq.update(1)\n",
        "\n",
        "           data, target = data.to(device).float(), target.to(device).float()\n",
        "\n",
        "           optimizer.zero_grad()\n",
        "\n",
        "           output_dict = model(data)\n",
        "\n",
        "           target_indexed = target.argmax(dim=1)\n",
        "\n",
        "           clsfy_out = output_dict\n",
        "           loss = combined_loss(clsfy_out, target_indexed)\n",
        "\n",
        "           loss.backward()\n",
        "           optimizer.step()\n",
        "\n",
        "\n",
        "           with torch.no_grad():\n",
        "               pred_idx = clsfy_out.argmax(dim=1)\n",
        "\n",
        "\n",
        "               tp, fp, fn, tn = smp.metrics.get_stats(pred_idx, target_indexed, mode='multiclass', num_classes=num_classes)\n",
        "\n",
        "\n",
        "               iou_scores = smp.metrics.iou_score(tp, fp, fn, tn, reduction='none')\n",
        "               dice_scores = smp.metrics.f1_score(tp, fp, fn, tn, reduction='none')\n",
        "\n",
        "\n",
        "               for i in range(iou_scores.size(0)):\n",
        "\n",
        "                   if i < num_classes:\n",
        "                       iou_per_class[i].update(iou_scores[i].cpu(), weight=data.shape[0])\n",
        "                   if i < num_classes:\n",
        "                       dice_per_class[i].update(dice_scores[i].cpu(), weight=data.shape[0])\n",
        "\n",
        "\n",
        "               loss_record.update(loss.detach().cpu(), weight=data.shape[0])\n",
        "\n",
        "\n",
        "           tq.set_postfix_str(s=f\"Loss: {loss_record.compute():.4f}\")\n",
        "\n",
        "\n",
        "   epoch_loss = loss_record.compute()\n",
        "   epoch_iou_per_class = {i: iou_per_class[i].compute() for i in range(num_classes)}\n",
        "   epoch_dice_per_class = {i: dice_per_class[i].compute() for i in range(num_classes)}\n",
        "\n",
        "\n",
        "   return epoch_loss, epoch_iou_per_class, epoch_dice_per_class\n",
        "\n",
        "\n",
        "def McDropout(model, x, mc_runs=20):\n",
        "    # return mean and variance probability using Monte-Carlo Dropout.\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    def enable_dropout(m):\n",
        "        if isinstance(m, (nn.Dropout, nn.Dropout3d)):\n",
        "            m.train()\n",
        "    model.apply(enable_dropout)\n",
        "\n",
        "    probs = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(mc_runs):\n",
        "            logits = model(x)\n",
        "            probs.append(torch.softmax(logits, dim=1))\n",
        "\n",
        "    stack = torch.stack(probs, dim=0)\n",
        "    mean  = stack.mean(dim=0)\n",
        "    var   = stack.var (dim=0)\n",
        "    return mean, var\n",
        "\n",
        "# Validation function\n",
        "def ValidateModel(\n",
        "   model,\n",
        "   loader,\n",
        "   device,\n",
        "   num_classes,\n",
        "   epoch_index,\n",
        "   total_epochs,\n",
        "   writer=None\n",
        "):\n",
        "   model.eval()\n",
        "\n",
        "\n",
        "   loss_record = MeanMetric()\n",
        "   iou_per_class = {i: MeanMetric() for i in range(num_classes)}\n",
        "   dice_per_class = {i: MeanMetric() for i in range(num_classes)}\n",
        "   mlc_acc = MulticlassAccuracy(num_classes=num_classes, average='macro')\n",
        "\n",
        "\n",
        "   loader_len = len(loader)\n",
        "\n",
        "   if writer is not None:\n",
        "    try:\n",
        "        data, _ = next(iter(val_loader))\n",
        "        data = data.to(device).float()\n",
        "\n",
        "        mean_prob_sample, var_prob_sample = McDropout(model, data[:1], mc_runs=20)\n",
        "        model.eval()\n",
        "\n",
        "\n",
        "        z = var_prob_sample.shape[2] // 2\n",
        "        heat = var_prob_sample[0, 1, z, :, :]\n",
        "\n",
        "\n",
        "        heat = (heat - heat.min()) / (heat.max() - heat.min() + 1e-8)\n",
        "\n",
        "        writer.add_image(\"uncertainty/class1\",\n",
        "                         heat.cpu().numpy(),\n",
        "                         global_step=epoch_index,\n",
        "                         dataformats=\"HW\")\n",
        "        writer.flush()\n",
        "    except StopIteration:\n",
        "        print(\"Validation loader is empty.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: uncertainty map to TensorBoard: {e}\")\n",
        "\n",
        "\n",
        "   with tqdm(total=loader_len, ncols=122) as tq:\n",
        "       tq.set_description(f\"Valid :: Epoch: {epoch_index}/{total_epochs}\")\n",
        "\n",
        "       for data, target in loader:\n",
        "           tq.update(1)\n",
        "\n",
        "           data, target = data.to(device).float(), target.to(device).float()\n",
        "\n",
        "           with torch.no_grad():\n",
        "               output_dict = model(data)\n",
        "\n",
        "               mean_prob, _ = McDropout(model, data, mc_runs=20)\n",
        "               mc_pred_idx = mean_prob.argmax(dim=1)\n",
        "\n",
        "\n",
        "           clsfy_out = output_dict\n",
        "           target_indexed = target.argmax(dim=1)\n",
        "\n",
        "           loss = combined_loss(clsfy_out, target_indexed)\n",
        "           pred_idx = clsfy_out.argmax(dim=1)\n",
        "\n",
        "\n",
        "           tp, fp, fn, tn = smp.metrics.get_stats(pred_idx, target_indexed, mode='multiclass', num_classes=num_classes)\n",
        "\n",
        "\n",
        "           iou_scores = smp.metrics.iou_score(tp, fp, fn, tn, reduction='none')\n",
        "           dice_scores = smp.metrics.f1_score(tp, fp, fn, tn, reduction='none')\n",
        "\n",
        "\n",
        "           for i in range(iou_scores.size(0)):\n",
        "\n",
        "               if i < num_classes:\n",
        "                   iou_per_class[i].update(iou_scores[i].cpu(), weight=data.shape[0])\n",
        "               if i < num_classes:\n",
        "                   dice_per_class[i].update(dice_scores[i].cpu(), weight=data.shape[0])\n",
        "\n",
        "           mlc_acc.update(mc_pred_idx.cpu(), target_indexed.cpu())\n",
        "           loss_record.update(loss.cpu(), weight=data.shape[0])\n",
        "           tq.set_postfix_str(s=f\"Loss: {loss_record.compute():.4f}\")\n",
        "\n",
        "\n",
        "       valid_epoch_loss = loss_record.compute()\n",
        "       valid_epoch_iou_per_class = {i: iou_per_class[i].compute() for i in range(num_classes)}\n",
        "       valid_epoch_dice_per_class = {i: dice_per_class[i].compute() for i in range(num_classes)}\n",
        "       valid_mc_acc = mlc_acc.compute()\n",
        "\n",
        "\n",
        "   return valid_epoch_loss, valid_epoch_iou_per_class, valid_epoch_dice_per_class, valid_mc_acc\n",
        "\n",
        "\n",
        "# Main function\n",
        "def Main(*, model, optimizer, ckpt_dir, pin_memory=True, device=\"gpu\"):\n",
        "\n",
        "    total_epochs = TrainingConfig.EPOCHS\n",
        "    num_classes = 4\n",
        "\n",
        "    model.to(device, non_blocking=True)\n",
        "\n",
        "    writer = SummaryWriter(log_dir=os.path.join(ckpt_dir, \"tboard_logs\"))\n",
        "    best_loss = float(\"inf\")\n",
        "\n",
        "    live_plot = PlotLosses(outputs=[MatplotlibPlot(cell_size=(8, 3)), ExtremaPrinter()])\n",
        "\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=total_epochs)\n",
        "\n",
        "\n",
        "    for epoch in range(total_epochs):\n",
        "        current_epoch = epoch + 1\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "        train_loss, train_iou_per_class, train_dice_per_class = TrainModel(\n",
        "            model=model,\n",
        "            loader=train_loader,\n",
        "            optimizer=optimizer,\n",
        "            num_classes=num_classes,\n",
        "            device=device,\n",
        "            epoch_index=current_epoch,\n",
        "            total_epochs=total_epochs,\n",
        "        )\n",
        "\n",
        "\n",
        "        valid_loss, valid_iou_per_class, valid_dice_per_class, valid_mc_acc = ValidateModel(\n",
        "            model=model,\n",
        "            loader=val_loader,\n",
        "            device=device,\n",
        "            num_classes=num_classes,\n",
        "            epoch_index=current_epoch,\n",
        "            total_epochs=total_epochs,\n",
        "            writer=writer\n",
        "        )\n",
        "\n",
        "\n",
        "        # create plots\n",
        "        plot_metrics = {\n",
        "            \"loss\": train_loss,\n",
        "            \"val_loss\": valid_loss,\n",
        "            \"val_mc_accuracy\": valid_mc_acc,\n",
        "        }\n",
        "        for i in range(num_classes):\n",
        "            plot_metrics[f\"IoU_Class_{i}\"] = train_iou_per_class[i].item()\n",
        "            plot_metrics[f\"val_IoU_Class_{i}\"] = valid_iou_per_class[i].item()\n",
        "            plot_metrics[f\"Dice_Class_{i}\"] = train_dice_per_class[i].item()\n",
        "            plot_metrics[f\"val_Dice_Class_{i}\"] = valid_dice_per_class[i].item()\n",
        "\n",
        "        live_plot.update(plot_metrics)\n",
        "\n",
        "        live_plot.send()\n",
        "\n",
        "\n",
        "        writer.add_scalar(\"Loss/train\", train_loss, current_epoch)\n",
        "        writer.add_scalar(\"Loss/valid\", valid_loss, current_epoch)\n",
        "        writer.add_scalar(\"MC_Accuracy/valid\", valid_mc_acc, current_epoch)\n",
        "\n",
        "\n",
        "        for i in range(num_classes):\n",
        "            writer.add_scalar(f\"IoU_Class_{i}/train\", train_iou_per_class[i].item(), current_epoch)\n",
        "            writer.add_scalar(f\"IoU_Class_{i}/valid\", valid_iou_per_class[i].item(), current_epoch)\n",
        "            writer.add_scalar(f\"Dice_Class_{i}/train\", train_dice_per_class[i].item(), current_epoch)\n",
        "            writer.add_scalar(f\"Dice_Class_{i}/valid\", valid_dice_per_class[i].item(), current_epoch)\n",
        "\n",
        "\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Save the model if validation loss improves\n",
        "        if valid_loss < best_loss:\n",
        "            best_loss = valid_loss\n",
        "            print(\"Model Improved. Saving...\", end=\"\")\n",
        "\n",
        "            checkpoint_dict = {\n",
        "                \"opt\": optimizer.state_dict(),\n",
        "                \"model\": model.state_dict(),\n",
        "            }\n",
        "            torch.save(checkpoint_dict, os.path.join(ckpt_dir, \"ckpt.tar\"))\n",
        "            del checkpoint_dict\n",
        "            print(\"Done.\\n\")\n",
        "\n",
        "    writer.close()\n",
        "    return"
      ],
      "metadata": {
        "id": "rpSaFOtKcLSO"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to train the model\n",
        "Main(\n",
        "   model = model,\n",
        "   optimizer = optimizer,\n",
        "   ckpt_dir = CKPT_DIR,\n",
        "   device  = DEVICE,\n",
        "   pin_memory = GPU_AVAILABLE\n",
        ")"
      ],
      "metadata": {
        "id": "QlJXGRZ3jCC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test the model**"
      ],
      "metadata": {
        "id": "_Fj9i1BmGeVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test function\n",
        "def TestModel(\n",
        "   model,\n",
        "   loader,\n",
        "   device,\n",
        "   num_classes\n",
        "):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    loss_record = MeanMetric()\n",
        "    iou_per_class = {i: MeanMetric() for i in range(num_classes)}\n",
        "    dice_per_class = {i: MeanMetric() for i in range(num_classes)}\n",
        "    mlc_acc = MulticlassAccuracy(num_classes=num_classes, average='macro')\n",
        "\n",
        "\n",
        "    loader_len = len(loader)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        with tqdm(total=loader_len, ncols=122) as tq:\n",
        "            tq.set_description(f\"Test :: Evaluation\")\n",
        "\n",
        "            for data, target in loader:\n",
        "                tq.update(1)\n",
        "\n",
        "                data, target = data.to(device).float(), target.to(device).float()\n",
        "\n",
        "                output_dict = model(data)\n",
        "\n",
        "                mean_prob, _ = McDropout(model, data, mc_runs=20)\n",
        "                mc_pred_idx = mean_prob.argmax(dim=1)\n",
        "\n",
        "\n",
        "                clsfy_out = output_dict\n",
        "                target_indexed = target.argmax(dim=1)\n",
        "\n",
        "\n",
        "                loss = combined_loss(clsfy_out, target_indexed)\n",
        "                pred_idx = clsfy_out.argmax(dim=1)\n",
        "\n",
        "\n",
        "                tp, fp, fn, tn = smp.metrics.get_stats(pred_idx, target_indexed, mode='multiclass', num_classes=num_classes)\n",
        "\n",
        "\n",
        "                iou_scores = smp.metrics.iou_score(tp, fp, fn, tn, reduction='none')\n",
        "                dice_scores = smp.metrics.f1_score(tp, fp, fn, tn, reduction='none')\n",
        "\n",
        "\n",
        "                for i in range(iou_scores.size(0)):\n",
        "                   if i < num_classes:\n",
        "                       iou_per_class[i].update(iou_scores[i].cpu(), weight=data.shape[0])\n",
        "                   if i < num_classes:\n",
        "                       dice_per_class[i].update(dice_scores[i].cpu(), weight=data.shape[0])\n",
        "\n",
        "\n",
        "                # update Monte Carlo accuracy\n",
        "                mlc_acc.update(mc_pred_idx.cpu(), target_indexed.cpu())\n",
        "                loss_record.update(loss.cpu(), weight=data.shape[0])\n",
        "\n",
        "\n",
        "            test_loss = loss_record.compute()\n",
        "            test_iou_per_class = {i: iou_per_class[i].compute() for i in range(num_classes)}\n",
        "            test_dice_per_class = {i: dice_per_class[i].compute() for i in range(num_classes)}\n",
        "            test_mc_acc = mlc_acc.compute()\n",
        "\n",
        "\n",
        "    print(\"Test Set Evaluation Results\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"Test Macro Carlo Accuracy: {test_mc_acc:.4f}\")\n",
        "    print(\"Test IoU per class:\")\n",
        "    for i in range(num_classes):\n",
        "        class_name = class_names[i] if 'class_names' in globals() and i < len(class_names) else f\"Class {i}\"\n",
        "        print(f\"  {class_name}: {test_iou_per_class[i].item():.4f}\")\n",
        "    print(\"Test Dice per class:\")\n",
        "    for i in range(num_classes):\n",
        "        class_name = class_names[i] if 'class_names' in globals() and i < len(class_names) else f\"Class {i}\"\n",
        "        print(f\"  {class_name}: {test_dice_per_class[i].item():.4f}\")\n",
        "\n",
        "    print(\"Completed!\")\n",
        "\n",
        "    return test_loss, test_iou_per_class, test_dice_per_class, test_mc_acc\n",
        "\n",
        "\n",
        "num_classes = 4\n",
        "if 'class_names' not in globals():\n",
        "     class_names = ['Class 0', 'Class 1', 'Class 2', 'Class 3'] # Default names"
      ],
      "metadata": {
        "id": "V4TAGhB6qa3p"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Display"
      ],
      "metadata": {
        "id": "DwU-o_Aws5O-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_default_device():\n",
        "   gpu_available = torch.cuda.is_available()\n",
        "   return torch.device('cuda' if gpu_available else 'cpu'), gpu_available\n",
        "\n",
        "DEVICE, GPU_AVAILABLE = get_default_device()\n",
        "\n",
        "trained_model = Unet(in_channels = 3, out_channels = 4)\n",
        "trained_model.load_state_dict(torch.load(\"/content/drive/MyDrive/MSc_Project/3D_Brain_Images/version_26/ckpt.tar\", map_location = DEVICE)['model'])\n",
        "trained_model.to(DEVICE)\n",
        "trained_model.eval()"
      ],
      "metadata": {
        "id": "7ZrLx-YyU0Kt",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the test function\n",
        "test_loss, test_iou, test_dice, test_mc_acc = TestModel(\n",
        "    model=trained_model,\n",
        "    loader=test_loader,\n",
        "    device=DEVICE,\n",
        "    num_classes=num_classes\n",
        ")"
      ],
      "metadata": {
        "id": "2w0lWZeQU9o2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3b85525"
      },
      "source": [
        "# **Visualization**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "690cccc6"
      },
      "source": [
        "## Set up the directory structure\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "297c8c29",
        "collapsed": true
      },
      "source": [
        "\n",
        "qualitative_examples = []\n",
        "num_examples_to_select = 5\n",
        "\n",
        "for i, (images, masks) in enumerate(test_loader):\n",
        "\n",
        "    images = images.to(DEVICE).float()\n",
        "    masks = masks.to(DEVICE).float()\n",
        "\n",
        "\n",
        "    if i < num_examples_to_select:\n",
        "        qualitative_examples.append({'image': images[0], 'mask': masks[0]})\n",
        "        print(f\"Selected example {i+1}\")\n",
        "    else:\n",
        "        break\n",
        "\n",
        "\n",
        "\n",
        "base_save_dir = \"/content/drive/MyDrive/MSc_Project/3D_Brain_Images/version_31\"\n",
        "visualization_dir = os.path.join(base_save_dir, \"3D_brain_vis\")\n",
        "\n",
        "\n",
        "os.makedirs(visualization_dir, exist_ok=True)\n",
        "\n",
        "# Create subfolders for each patient and plane\n",
        "print(f\"Creating patient and plane directories inside: {visualization_dir}\")\n",
        "\n",
        "planes = ['Axial', 'Coronal', 'Sagittal']\n",
        "\n",
        "\n",
        "if qualitative_examples:\n",
        "    for example_idx in range(len(qualitative_examples)):\n",
        "\n",
        "        patient_dir = os.path.join(visualization_dir, f\"patient_{example_idx + 1}\")\n",
        "        os.makedirs(patient_dir, exist_ok=True)\n",
        "        print(f\"Patient directory complete: {patient_dir}\")\n",
        "\n",
        "\n",
        "        for plane_name in planes:\n",
        "            plane_dir = os.path.join(patient_dir, plane_name)\n",
        "            os.makedirs(plane_dir, exist_ok=True)\n",
        "            print(f\"Plane directorycomplete: {plane_dir}\")\n",
        "else:\n",
        "    print(\"Error occured\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modality_names = ['T1ce', 'FLAIR', 'T2']\n",
        "class_names = ['Background', 'Necrotic Core', 'Edema', 'Enhancing Tumor']\n",
        "classes_visualize_indices = [1, 2, 3]\n",
        "classes_visualize_names = [class_names[i] for i in classes_visualize_indices]\n",
        "temp_num_slices_visualize = 5\n",
        "num_classes_to_visualize = len(classes_visualize_indices)\n",
        "num_total_classes = len(class_names)\n",
        "\n",
        "if 'visualization_dir' not in locals():\n",
        "      print(\"Warning: visualization directory not defined.\")\n",
        "      print('Using the default.')\n",
        "      visualization_dir = \"/content/drive/MyDrive/MSc_Project/3D_Brain_Images/version_18/3D_brain_vis\"\n",
        "      os.makedirs(visualization_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "planes_to_visualize = {\n",
        "    'Axial': {'dim': 2, 'title': 'Axial', 'slice_indices_func': lambda shape, n_slices: np.linspace(0, shape - 1, n_slices, dtype=int)},\n",
        "    'Coronal': {'dim': 3, 'title': 'Coronal', 'slice_indices_func': lambda shape, n_slices: np.linspace(0, shape - 1, n_slices, dtype=int)},\n",
        "    'Sagittal': {'dim': 4, 'title': 'Sagittal', 'slice_indices_func': lambda shape, n_slices: np.linspace(0, shape - 1, n_slices, dtype=int)}\n",
        "}\n"
      ],
      "metadata": {
        "id": "XGKe4uBv2_xm"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61ca5ca6"
      },
      "source": [
        "**Generate combined mri/mask/predicted figures**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9ff1384",
        "collapsed": true
      },
      "source": [
        "legend_patches = [mpatches.Patch(color=plt.cm.viridis(i / (num_total_classes - 1)), label=class_names[i])\n",
        "                  for i in range(num_total_classes)]\n",
        "\n",
        "\n",
        "if 'qualitative_examples' not in locals() or not qualitative_examples:\n",
        "    print(\"Error\")\n",
        "else:\n",
        "    for example_idx, example in enumerate(qualitative_examples):\n",
        "        trained_model.eval()\n",
        "\n",
        "        image = example['image'].unsqueeze(0).to(DEVICE).float()\n",
        "        mask = example['mask'].unsqueeze(0).to(DEVICE).float()\n",
        "\n",
        "        print(f\"Generating Visualization for Example {example_idx + 1}/{len(qualitative_examples)}\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predicted_logits = trained_model(image)\n",
        "            predicted_mask = predicted_logits.argmax(dim=1)\n",
        "\n",
        "\n",
        "        for plane_name, plane_info in planes_to_visualize.items():\n",
        "            slice_dim_tensor = plane_info['dim']\n",
        "            plane_title = plane_info['title']\n",
        "            slice_indices_func = plane_info['slice_indices_func']\n",
        "\n",
        "            total_slices_in_dim = image.shape[slice_dim_tensor]\n",
        "            slice_indices = slice_indices_func(total_slices_in_dim, temp_num_slices_visualize)\n",
        "\n",
        "            if len(slice_indices) < temp_num_slices_visualize:\n",
        "                  print(f\"Not enough slices in {plane_name}.\")\n",
        "                  num_slices_visualize = len(slice_indices)\n",
        "            else:\n",
        "                  num_slices_visualize = temp_num_slices_visualize\n",
        "\n",
        "\n",
        "            print(f\"Generating {plane_name} view.\")\n",
        "\n",
        "            patient_plane_dir = os.path.join(visualization_dir, f\"patient_{example_idx + 1}\", plane_name)\n",
        "            os.makedirs(patient_plane_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "            image_slices_combined = []\n",
        "            mask_slices_combined = []\n",
        "            predicted_mask_slices_list_combined = []\n",
        "\n",
        "            for slice_idx in slice_indices:\n",
        "                  if slice_dim_tensor == 2: # Axial (slice along D)\n",
        "                      image_slices_combined.append(image.squeeze(0)[:, slice_idx, :, :])\n",
        "                      mask_slices_combined.append(mask.squeeze(0)[:, slice_idx, :, :])\n",
        "                      predicted_mask_slices_list_combined.append(predicted_mask.squeeze(0)[slice_idx, :, :])\n",
        "                  elif slice_dim_tensor == 3: # Coronal (slice along H)\n",
        "                      image_slices_combined.append(image.squeeze(0)[:, :, slice_idx, :])\n",
        "                      mask_slices_combined.append(mask.squeeze(0)[:, :, slice_idx, :])\n",
        "                      predicted_mask_slices_list_combined.append(predicted_mask.squeeze(0)[:, slice_idx, :])\n",
        "                  elif slice_dim_tensor == 4: # Sagittal (slice along W)\n",
        "                      image_slices_combined.append(image.squeeze(0)[:, :, :, slice_idx])\n",
        "                      mask_slices_combined.append(mask.squeeze(0)[:, :, :, slice_idx])\n",
        "                      predicted_mask_slices_list_combined.append(predicted_mask.squeeze(0)[:, :, slice_idx])\n",
        "\n",
        "\n",
        "            image_slices_combined = torch.stack(image_slices_combined, dim=1)\n",
        "            mask_slices_combined = torch.stack(mask_slices_combined, dim=1)\n",
        "            predicted_mask_slices_combined = torch.stack(predicted_mask_slices_list_combined, dim=0)\n",
        "            mask_slices_indexed_combined = torch.argmax(mask_slices_combined, dim=0)\n",
        "\n",
        "\n",
        "\n",
        "            num_rows_combined = 3 + 1 + 1\n",
        "            num_cols_combined = num_slices_visualize\n",
        "\n",
        "            fig_combined, axes_combined = plt.subplots(num_rows_combined, num_cols_combined, figsize=(num_cols_combined * 3.5, num_rows_combined * 3))\n",
        "\n",
        "            for s_plot_idx, s_orig_idx in enumerate(slice_indices):\n",
        "                # Plot  Modalities\n",
        "                for m_idx in range(image_slices_combined.shape[0]):\n",
        "                    ax = axes_combined[m_idx, s_plot_idx]\n",
        "                    ax.imshow(image_slices_combined[m_idx, s_plot_idx, :, :].cpu().numpy(), cmap='gray')\n",
        "                    if s_plot_idx == 0: ax.set_ylabel(modality_names[m_idx], rotation=90, size='large')\n",
        "                    if m_idx == 0: ax.set_title(f'{plane_title} Slice {s_orig_idx}', size='large')\n",
        "                    ax.axis('off')\n",
        "\n",
        "                # Plot Ground Truth Mask\n",
        "                ax = axes_combined[3, s_plot_idx]\n",
        "                ax.imshow(mask_slices_indexed_combined[s_plot_idx, :, :].cpu().numpy(), cmap='viridis')\n",
        "                if s_plot_idx == 0: ax.set_ylabel('Ground Truth', rotation=90, size='large')\n",
        "                ax.axis('off')\n",
        "\n",
        "                # Plot Predicted Mask\n",
        "                ax = axes_combined[4, s_plot_idx]\n",
        "                ax.imshow(predicted_mask_slices_combined[s_plot_idx, :, :].cpu().numpy(), cmap='viridis')\n",
        "                if s_plot_idx == 0: ax.set_ylabel('Prediction', rotation=90, size='large')\n",
        "                ax.axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            fig_combined.suptitle(f'Patient {example_idx + 1} - {plane_title} View - Combined', y=1.02, fontsize=16)\n",
        "\n",
        "\n",
        "            fig_combined.legend(handles=legend_patches, loc='lower center', bbox_to_anchor=(0.5, -0.05),\n",
        "                                fancybox=True, shadow=True, ncol=num_total_classes)\n",
        "\n",
        "            fig_combined_save_path = os.path.join(patient_plane_dir, f\"combined_visualization_{plane_name.lower()}.png\")\n",
        "            plt.savefig(fig_combined_save_path, bbox_inches='tight')\n",
        "            plt.close(fig_combined)\n",
        "            print(f\"Saved combined visualization\")\n",
        "\n",
        "\n",
        "    print(\"Completed!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d338a835"
      },
      "source": [
        "**Softmax probability map**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df7745c9",
        "collapsed": true
      },
      "source": [
        "if 'qualitative_examples' not in locals() or not qualitative_examples:\n",
        "    print(\"Error\")\n",
        "else:\n",
        "    for example_idx, example in enumerate(qualitative_examples):\n",
        "        trained_model.eval()\n",
        "\n",
        "        image = example['image'].unsqueeze(0).to(DEVICE).float()\n",
        "        mask = example['mask'].unsqueeze(0).to(DEVICE).float()\n",
        "\n",
        "        print(f\"Generating Visualization.\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predicted_logits = trained_model(image)\n",
        "\n",
        "            softmax_probs = F.softmax(predicted_logits, dim=1)\n",
        "\n",
        "\n",
        "        for plane_name, plane_info in planes_to_visualize.items():\n",
        "            slice_dim_tensor = plane_info['dim']\n",
        "            plane_title = plane_info['title']\n",
        "            slice_indices_func = plane_info['slice_indices_func']\n",
        "\n",
        "            total_slices_in_dim = image.shape[slice_dim_tensor]\n",
        "            slice_indices = slice_indices_func(total_slices_in_dim, temp_num_slices_visualize)\n",
        "\n",
        "            if len(slice_indices) < temp_num_slices_visualize:\n",
        "                 print(f\"Not enough slices in {plane_name}.\")\n",
        "                 num_slices_visualize = len(slice_indices)\n",
        "            else:\n",
        "                 num_slices_visualize = temp_num_slices_visualize\n",
        "\n",
        "\n",
        "            print(f\"Generating {plane_name} view.\")\n",
        "\n",
        "            patient_plane_dir = os.path.join(visualization_dir, f\"patient_{example_idx + 1}\", plane_name)\n",
        "            os.makedirs(patient_plane_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "            softmax_slices_list = []\n",
        "\n",
        "            for slice_idx in slice_indices:\n",
        "                 if slice_dim_tensor == 2:\n",
        "                     softmax_slices_list.append(softmax_probs.squeeze(0)[:, slice_idx, :, :])\n",
        "                 elif slice_dim_tensor == 3:\n",
        "                      softmax_slices_list.append(softmax_probs.squeeze(0)[:, :, slice_idx, :])\n",
        "                 elif slice_dim_tensor == 4:\n",
        "                      softmax_slices_list.append(softmax_probs.squeeze(0)[:, :, :, slice_idx])\n",
        "\n",
        "            softmax_slices = torch.stack(softmax_slices_list, dim=1)\n",
        "\n",
        "            # Plotting for Softmax Probability Maps\n",
        "            num_rows_softmax = num_total_classes\n",
        "            num_cols_softmax = num_slices_visualize\n",
        "\n",
        "            fig_softmax, axes_softmax = plt.subplots(num_rows_softmax, num_cols_softmax, figsize=(num_cols_softmax * 3.5, num_rows_softmax * 3))\n",
        "\n",
        "\n",
        "            for s_plot_idx, s_orig_idx in enumerate(slice_indices):\n",
        "                 for c_idx in range(num_total_classes):\n",
        "                     ax = axes_softmax[c_idx, s_plot_idx]\n",
        "                     im = ax.imshow(softmax_slices[c_idx, s_plot_idx, :, :].cpu().numpy(), cmap='hot', vmin=0, vmax=1)\n",
        "                     if s_plot_idx == 0: ax.set_ylabel(f'Softmax ({class_names[c_idx]})', rotation=90, size='large')\n",
        "                     if c_idx == 0: ax.set_title(f'{plane_title} Slice {s_orig_idx}', size='large')\n",
        "                     ax.axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            fig_softmax.suptitle(f'Patient {example_idx + 1} - {plane_title} View - Softmax Probabilities', y=1.02, fontsize=16)\n",
        "\n",
        "\n",
        "            cbar_ax = fig_softmax.add_axes([1.0, 0.15, 0.02, 0.7])\n",
        "            cbar = Colorbar(ax = cbar_ax, mappable = im)\n",
        "            cbar.set_label('Probability')\n",
        "\n",
        "            fig_softmax_save_path = os.path.join(patient_plane_dir, f\"softmax_probabilities_{plane_name.lower()}.png\")\n",
        "            plt.savefig(fig_softmax_save_path, bbox_inches='tight')\n",
        "            plt.close(fig_softmax)\n",
        "            print(\"Saved softmax probabilities\")\n",
        "\n",
        "\n",
        "    print(\"Completed!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90ba3ea8"
      },
      "source": [
        "**Monte Carlo uncertanity map**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "812dc87b",
        "collapsed": true
      },
      "source": [
        "if 'qualitative_examples' not in locals() or not qualitative_examples:\n",
        "    print(\"Error\")\n",
        "else:\n",
        "    for example_idx, example in enumerate(qualitative_examples):\n",
        "        trained_model.eval()\n",
        "\n",
        "        image = example['image'].unsqueeze(0).to(DEVICE).float()\n",
        "        mask = example['mask'].unsqueeze(0).to(DEVICE).float()\n",
        "\n",
        "        print(\"Generating Visualization\")\n",
        "\n",
        "\n",
        "        mean_prob, var_prob = McDropout(trained_model, image, mc_runs=20)\n",
        "        trained_model.eval()\n",
        "\n",
        "\n",
        "        uncertainty_maps = var_prob.squeeze(0)\n",
        "\n",
        "\n",
        "        for plane_name, plane_info in planes_to_visualize.items():\n",
        "            slice_dim_tensor = plane_info['dim']\n",
        "            plane_title = plane_info['title']\n",
        "            slice_indices_func = plane_info['slice_indices_func']\n",
        "\n",
        "            total_slices_in_dim = image.shape[slice_dim_tensor]\n",
        "            slice_indices = slice_indices_func(total_slices_in_dim, temp_num_slices_visualize)\n",
        "\n",
        "            if len(slice_indices) < temp_num_slices_visualize:\n",
        "                 print(f\"Not enough slices in {plane_name}.\")\n",
        "                 num_slices_visualize = len(slice_indices)\n",
        "            else:\n",
        "                 num_slices_visualize = temp_num_slices_visualize\n",
        "\n",
        "\n",
        "            print(f\"Generating {plane_name} view\")\n",
        "\n",
        "            patient_plane_dir = os.path.join(visualization_dir, f\"patient_{example_idx + 1}\", plane_name)\n",
        "            os.makedirs(patient_plane_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "            image_slices_base = []\n",
        "            uncertainty_slices_dict_plane = {c: [] for c in classes_visualize_indices}\n",
        "\n",
        "\n",
        "            for slice_idx in slice_indices:\n",
        "                 if slice_dim_tensor == 2:\n",
        "                     image_slices_base.append(image.squeeze(0)[0, slice_idx, :, :])\n",
        "                     for c_idx in classes_visualize_indices:\n",
        "                         uncertainty_slices_dict_plane[c_idx].append(uncertainty_maps[c_idx, slice_idx, :, :])\n",
        "                 elif slice_dim_tensor == 3:\n",
        "                      image_slices_base.append(image.squeeze(0)[0, :, slice_idx, :])\n",
        "                      for c_idx in classes_visualize_indices:\n",
        "                          uncertainty_slices_dict_plane[c_idx].append(uncertainty_maps[c_idx, :, slice_idx, :])\n",
        "                 elif slice_dim_tensor == 4:\n",
        "                      image_slices_base.append(image.squeeze(0)[0, :, :, slice_idx])\n",
        "                      for c_idx in classes_visualize_indices:\n",
        "                          uncertainty_slices_dict_plane[c_idx].append(uncertainty_maps[c_idx, :, :, slice_idx])\n",
        "\n",
        "            image_slices_base = torch.stack(image_slices_base, dim=0)\n",
        "            uncertainty_slices_dict_plane = {c: torch.stack(v, dim=0) for c, v in uncertainty_slices_dict_plane.items()}\n",
        "\n",
        "\n",
        "            # Plots for Uncertainty Heatmaps\n",
        "\n",
        "            num_rows_uncertainty = num_classes_to_visualize\n",
        "            num_cols_uncertainty = num_slices_visualize\n",
        "\n",
        "            fig_uncertainty, axes_uncertainty = plt.subplots(num_rows_uncertainty, num_cols_uncertainty, figsize=(num_cols_uncertainty * 3.5, num_rows_uncertainty * 3))\n",
        "\n",
        "\n",
        "            all_uncertainty_slices = torch.cat(list(uncertainty_slices_dict_plane.values()), dim=0)\n",
        "            global_uncertainty_min = all_uncertainty_slices.min()\n",
        "            global_uncertainty_max = all_uncertainty_slices.max()\n",
        "\n",
        "\n",
        "            for s_plot_idx, s_orig_idx in enumerate(slice_indices):\n",
        "                for viz_class_plot_idx in range(num_classes_to_visualize):\n",
        "                    class_index = classes_visualize_indices[viz_class_plot_idx]\n",
        "                    ax = axes_uncertainty[viz_class_plot_idx, s_plot_idx]\n",
        "\n",
        "                    ax.imshow(image_slices_base[s_plot_idx, :, :].cpu().numpy(), cmap='gray')\n",
        "                    uncertainty_slice = uncertainty_slices_dict_plane[class_index][s_plot_idx, :, :]\n",
        "                    uncertainty_slice_normalized = (uncertainty_slice - global_uncertainty_min) / (global_uncertainty_max - global_uncertainty_min + 1e-8)\n",
        "                    im = ax.imshow(uncertainty_slice_normalized.cpu().numpy(), cmap='hot', alpha=0.5, vmin=0, vmax=1)\n",
        "\n",
        "                    if s_plot_idx == 0: ax.set_ylabel(f'Uncertainty ({classes_visualize_names[viz_class_plot_idx]})', rotation=90, size='large')\n",
        "                    if viz_class_plot_idx == 0: ax.set_title(f'{plane_title} Slice {s_orig_idx}', size='large')\n",
        "                    ax.axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            fig_uncertainty.suptitle(f'Patient {example_idx + 1} - {plane_title} View - Uncertainty Heatmaps', y=1.02, fontsize=16)\n",
        "\n",
        "\n",
        "            cbar_ax = fig_uncertainty.add_axes([1.0, 0.15, 0.02, 0.7])\n",
        "            cbar = Colorbar(ax = cbar_ax, mappable = im)\n",
        "            cbar.set_label('Normalized Variance')\n",
        "\n",
        "\n",
        "            uncertainty_fig_path = os.path.join(patient_plane_dir, f\"uncertainty_heatmaps_{plane_name.lower()}.png\")\n",
        "            plt.savefig(uncertainty_fig_path, bbox_inches='tight')\n",
        "            plt.close(fig_uncertainty)\n",
        "            print(\"Saved Uncertainty heatmaps\")\n",
        "\n",
        "\n",
        "    print(\"Completed!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21bb0f74"
      },
      "source": [
        "**Grad-CAM heatmap**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cea1f0da",
        "collapsed": true
      },
      "source": [
        "def ComputeGradcam(model, input_image, target_layer, target_class):\n",
        "    model.eval()\n",
        "    activations = []\n",
        "    gradients = []\n",
        "\n",
        "    def forward_hook(module, input, output):\n",
        "        activations.append(output)\n",
        "\n",
        "    def backward_hook(module, grad_input, grad_output):\n",
        "        gradients.append(grad_output[0])\n",
        "\n",
        "    hook_forward = target_layer.register_forward_hook(forward_hook)\n",
        "    hook_backward = target_layer.register_full_backward_hook(backward_hook)\n",
        "\n",
        "    output = model(input_image)\n",
        "    model.zero_grad()\n",
        "    score = output[0, target_class, ...].sum()\n",
        "    score.backward(retain_graph=True)\n",
        "\n",
        "    hook_forward.remove()\n",
        "    hook_backward.remove()\n",
        "\n",
        "    if not activations or not gradients:\n",
        "        print(\"Failed\")\n",
        "        return torch.zeros(input_image.shape[2], input_image.shape[3], input_image.shape[4], device=input_image.device)\n",
        "\n",
        "    capturedactivations = activations[0]\n",
        "    capturedgradients = gradients[0]\n",
        "    weights = torch.mean(capturedgradients, dim=[2, 3, 4], keepdim=True)\n",
        "    heatmap = torch.sum(weights * capturedactivations, dim=1, keepdim=True)\n",
        "    heatmap = F.relu(heatmap)\n",
        "    heatmap = F.interpolate(heatmap, size=input_image.shape[2:], mode='trilinear', align_corners=False)\n",
        "    heatmap_normalized = heatmap.squeeze()\n",
        "    heatmap_min, heatmap_max = heatmap_normalized.min(), heatmap_normalized.max()\n",
        "    if heatmap_max - heatmap_min > 1e-6:\n",
        "        heatmap_normalized = (heatmap_normalized - heatmap_min) / (heatmap_max - heatmap_min)\n",
        "    else:\n",
        "        heatmap_normalized = torch.zeros_like(heatmap_normalized)\n",
        "\n",
        "    return heatmap_normalized\n",
        "\n",
        "class GradCAM:\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "        self.activations = None\n",
        "        self.gradients = None\n",
        "        self.hook_handles_list = []\n",
        "        self.hook_handles_list.append(self.target_layer.register_forward_hook(self.save_activation))\n",
        "        self.hook_handles_list.append(self.target_layer.register_full_backward_hook(lambda module, grad_input, grad_output: self.save_gradient(grad_output[0])))\n",
        "\n",
        "    def save_activation(self, module, input, output):\n",
        "        self.activations = output.clone().detach()\n",
        "\n",
        "    def save_gradient(self, grad):\n",
        "        self.gradients = grad.clone().detach()\n",
        "\n",
        "    def compute_heatmap(self, input_image, target_class):\n",
        "        input_image.requires_grad_(True)\n",
        "        output = self.model(input_image)\n",
        "        self.model.zero_grad()\n",
        "        score = output[0, target_class, ...].sum()\n",
        "        score.backward(retain_graph=True)\n",
        "\n",
        "        if self.activations is not None and self.gradients is not None:\n",
        "            weights = torch.mean(self.gradients, dim=[2, 3, 4], keepdim=True)\n",
        "            grad_cam = torch.sum(weights * self.activations, dim=1, keepdim=True)\n",
        "            grad_cam = F.relu(grad_cam)\n",
        "            target_size = (input_image.shape[2], input_image.shape[3], input_image.shape[4])\n",
        "            grad_cam_resized = F.interpolate(grad_cam, size=target_size, mode='trilinear', align_corners=False)\n",
        "            grad_cam_normalized = grad_cam_resized.squeeze(0).squeeze(0)\n",
        "            grad_cam_min = grad_cam_normalized.min()\n",
        "            grad_cam_max = grad_cam_normalized.max()\n",
        "            if grad_cam_max - grad_cam_min > 1e-6:\n",
        "                grad_cam_normalized = (grad_cam_normalized - grad_cam_min) / (grad_cam_max - grad_cam_min)\n",
        "            else:\n",
        "                grad_cam_normalized = torch.zeros_like(grad_cam_normalized)\n",
        "            self.activations = None\n",
        "            self.gradients = None\n",
        "            return grad_cam_normalized\n",
        "        else:\n",
        "             print(f\"Cannot compute Grad-CAM.\")\n",
        "             return torch.zeros(input_image.shape[2], input_image.shape[3], input_image.shape[4], device=input_image.device)\n",
        "\n",
        "    def __del__(self):\n",
        "        for handle in self.hook_handles_list:\n",
        "            handle.remove()\n",
        "\n",
        "\n",
        "\n",
        "if 'model' not in locals() or trained_model is None:\n",
        "    print(\"Error\")\n",
        "    target_layer_for_gradcam = None\n",
        "    target_layer_name_for_gradcam = \"Unknown\"\n",
        "else:\n",
        "    try:\n",
        "        target_layer_for_gradcam = trained_model.conv4[4]\n",
        "        target_layer_name_for_gradcam = 'conv4.4'\n",
        "        print(f\"Using target layer for Grad-CAM: {target_layer_name_for_gradcam}\")\n",
        "    except AttributeError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        target_layer_for_gradcam = None\n",
        "        target_layer_name_for_gradcam = \"Unknown\"\n",
        "\n",
        "\n",
        "if target_layer_for_gradcam is None:\n",
        "    print(\"Skipping visualization due to invalid target layer.\")\n",
        "else:\n",
        "\n",
        "    # Instantiate GradCAM\n",
        "    gradcam_analyzer = GradCAM(trained_model, target_layer_for_gradcam)\n",
        "\n",
        "    if 'qualitative_examples' not in locals() or not qualitative_examples:\n",
        "        print(\"Error\")\n",
        "    else:\n",
        "        for example_idx, example in enumerate(qualitative_examples):\n",
        "            trained_model.eval()\n",
        "\n",
        "            image = example['image'].unsqueeze(0).to(DEVICE).float()\n",
        "            mask = example['mask'].unsqueeze(0).to(DEVICE).float()\n",
        "\n",
        "            print(\"Generating Visualization.\")\n",
        "\n",
        "            # Compute Grad-CAM\n",
        "\n",
        "            image_for_gradcam = image.clone().to(DEVICE).requires_grad_(True)\n",
        "\n",
        "            grad_cam_heatmaps = {}\n",
        "            for class_index in classes_visualize_indices:\n",
        "                grad_cam_heatmaps[class_index] = ComputeGradcam(\n",
        "                    trained_model,\n",
        "                    image_for_gradcam,\n",
        "                    target_layer_for_gradcam,\n",
        "                    class_index\n",
        "                )\n",
        "\n",
        "\n",
        "            for plane_name, plane_info in planes_to_visualize.items():\n",
        "                slice_dim_tensor = plane_info['dim']\n",
        "                plane_title = plane_info['title']\n",
        "                slice_indices_func = plane_info['slice_indices_func']\n",
        "\n",
        "                total_slices_in_dim = image.shape[slice_dim_tensor]\n",
        "                slice_indices = slice_indices_func(total_slices_in_dim, temp_num_slices_visualize)\n",
        "\n",
        "                if len(slice_indices) < temp_num_slices_visualize:\n",
        "                     print(f\"Not enough slices in {plane_name}.\")\n",
        "                     num_slices_visualize = len(slice_indices)\n",
        "                else:\n",
        "                     num_slices_visualize = temp_num_slices_visualize\n",
        "\n",
        "\n",
        "                print(f\"Generating {plane_name} view.\")\n",
        "\n",
        "                patient_plane_dir = os.path.join(visualization_dir, f\"patient_{example_idx + 1}\", plane_name)\n",
        "                os.makedirs(patient_plane_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "                image_slices_base = []\n",
        "                grad_cam_slices_dict_plane = {c: [] for c in classes_visualize_indices}\n",
        "\n",
        "                for slice_idx in slice_indices:\n",
        "                     if slice_dim_tensor == 2:\n",
        "                         image_slices_base.append(image.squeeze(0)[0, slice_idx, :, :])\n",
        "                         for c_idx in classes_visualize_indices:\n",
        "                             grad_cam_slices_dict_plane[c_idx].append(grad_cam_heatmaps[c_idx][slice_idx, :, :])\n",
        "                     elif slice_dim_tensor == 3:\n",
        "                          image_slices_base.append(image.squeeze(0)[0, :, slice_idx, :])\n",
        "                          for c_idx in classes_visualize_indices:\n",
        "                              grad_cam_slices_dict_plane[c_idx].append(grad_cam_heatmaps[c_idx][:, slice_idx, :])\n",
        "                     elif slice_dim_tensor == 4:\n",
        "                          image_slices_base.append(image.squeeze(0)[0, :, :, slice_idx])\n",
        "                          for c_idx in classes_visualize_indices:\n",
        "                              grad_cam_slices_dict_plane[c_idx].append(grad_cam_heatmaps[c_idx][:, :, slice_idx])\n",
        "\n",
        "                image_slices_base = torch.stack(image_slices_base, dim=0)\n",
        "                grad_cam_slices_dict_plane = {c: torch.stack(v, dim=0) for c, v in grad_cam_slices_dict_plane.items()}\n",
        "\n",
        "\n",
        "                # Plots for Grad-CAM Heatmaps\n",
        "\n",
        "                num_rows_gradcam = num_classes_to_visualize\n",
        "                num_cols_gradcam = num_slices_visualize\n",
        "\n",
        "                fig_gradcam, axes_gradcam = plt.subplots(num_rows_gradcam, num_cols_gradcam, figsize=(num_cols_gradcam * 3.5, num_rows_gradcam * 3))\n",
        "\n",
        "\n",
        "                all_gradcam_slices = torch.cat(list(grad_cam_slices_dict_plane.values()), dim=0)\n",
        "                global_gradcam_min = all_gradcam_slices.min()\n",
        "                global_gradcam_max = all_gradcam_slices.max()\n",
        "\n",
        "\n",
        "                for s_plot_idx, s_orig_idx in enumerate(slice_indices):\n",
        "                    for viz_class_plot_idx in range(num_classes_to_visualize):\n",
        "                         class_index = classes_visualize_indices[viz_class_plot_idx]\n",
        "                         ax = axes_gradcam[viz_class_plot_idx, s_plot_idx]\n",
        "                         ax.imshow(image_slices_base[s_plot_idx, :, :].cpu().numpy(), cmap='gray')\n",
        "                         grad_cam_slice = grad_cam_slices_dict_plane[class_index][s_plot_idx, :, :]\n",
        "                         grad_cam_slice_normalized = (grad_cam_slice - global_gradcam_min) / (global_gradcam_max - global_gradcam_min + 1e-8)\n",
        "                         im = ax.imshow(grad_cam_slice_normalized.cpu().detach().numpy(), cmap='hot', alpha=0.5, vmin=0, vmax=1)\n",
        "\n",
        "                         if s_plot_idx == 0: ax.set_ylabel(f'Grad-CAM ({classes_visualize_names[viz_class_plot_idx]})', rotation=90, size='large')\n",
        "                         if viz_class_plot_idx == 0: ax.set_title(f'{plane_title} Slice {s_orig_idx}', size='large')\n",
        "                         ax.axis('off')\n",
        "\n",
        "                plt.tight_layout()\n",
        "                fig_gradcam.suptitle(f'Patient {example_idx + 1} - {plane_title} View - Grad-CAM Heatmaps', y=1.02, fontsize=16)\n",
        "\n",
        "\n",
        "                cbar_ax = fig_gradcam.add_axes([1.0, 0.15, 0.02, 0.7])\n",
        "                cbar = Colorbar(ax = cbar_ax, mappable = im)\n",
        "                cbar.set_label('Importance')\n",
        "\n",
        "\n",
        "                gradcam_fig_path = os.path.join(patient_plane_dir, f\"gradcam_heatmaps_{plane_name.lower()}.png\")\n",
        "                plt.savefig(gradcam_fig_path, bbox_inches='tight')\n",
        "                plt.close(fig_gradcam)\n",
        "                print(\"Saved Grad-CAM heatmaps\")\n",
        "\n",
        "\n",
        "    print(\"Completed!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85db5fc4"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}